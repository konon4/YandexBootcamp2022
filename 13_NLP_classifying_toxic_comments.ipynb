{"cells": [{"cell_type": "markdown", "metadata": {"toc": true}, "source": ["<h1>Content<span class=\"tocSkip\"></span></h1>\n", "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Prepare\" data-toc-modified-id=\"Prepare-1\"><span class= \"toc-item-num\">1</span>Getting Ready</a></span><ul class=\"toc-item\"><li><span><a href=\"#Let's get-to-the-data\" data-toc-modified-id=\"Let's see-the-data-1.1\"><span class=\"toc-item-num\">1.1</span>Let's see the data</a></span></li> <li><span><a href=\"#Prepare-features\" data-toc-modified-id=\"Prepare-features-1.2\"><span class=\"toc-item-num\">1.2</span>Prepare features</a></span><li><span><a href=\"#Lemmatization-spacy\" data-toc-modified-id=\"Lemmatization-spacy-1.2.3\"><span class=\"toc- item-num\">1.2.3</span>Spacy Lemmatrization</a></span></li></ul></li><li><span><a href=\"#Prepare-selections\" data-toc-modified-id=\"Staging-selections-1.3\"><span class=\"toc-item-num\">1.3</span>Staging-selections</a></span><ul class=\"toc- item\"><li><span><a href=\"#Tokenization\" data-toc-modified-id=\"Tokenization-1.3.1\"><span class=\"toc-item-num\">1.3.1</ span>Tokenization</a></span></ li></ul></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2\"><span class= \"toc-item-num\">2</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#LogisticRegression-on-unbalanced- classes\" data-toc-modified-id=\"LogisticRegression-on-unbalanced-2.1-classes\"><span class=\"toc-item-num\">2.1</span>LogisticRegression-on-unbalanced-classes</a></span ></li><li><span><a href=\"#LogisticRegression-with-class_weight='balanced'\" data-toc-modified-id=\"LogisticRegression-with-class_weight='balanced'- 2.2\"><span class=\"toc-item-num\">2.2</span>LogisticRegression with class_weight='balanced'</a></span></li><li><span><a href= \"#DecisionTree-with-class_weight='balanced'\" data-toc-modified-id=\"DecisionTree-with-class_weight='balanced'-2.3\"><span class=\"toc-item-num\"> 2.3</span>DecisionTree with class_weight='balanced'</a></span></li><li><span><a href=\"#Let's try-balance-classes-via-smart select-num\" data-toc-modified-id=\"Let's-try-to-balance-classes-by-select-reduction-2.4\"><span class=\"toc-item-num\">2.4</span>Let's try to balance-classes-by-reduce samples</a></span></li><li><span><a href=\"#Logistic-regression-on-reduced-sample\" data-toc-modified-id=\"Logistic-regression-on- downsampled-2.5\"><span class=\"toc-item-num\">2.5</span>Reduced-sample logistic regression</a></span></li></ul></li>< li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-3\"><span class=\"toc-item-num\">3</span>Conclusions</a>< /span><ul class=\"toc-item\"><li><span><a href=\"#Check-list\" data-toc-modified-id=\"Check-list-4\">< span class=\"toc-item-num\">4</span>Checklist</a></span></li></ul></div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Project for \"Vykishop\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Online store \"Wikishop\" launches a new service. Now users can edit and supplement product descriptions, just like in wiki communities. That is, clients propose their edits and comment on the changes of others. The store needs a tool that will look for toxic comments and submit them for moderation.\n", "\n", "Train the model to classify comments into positive and negative. At your disposal is a dataset with markup on the toxicity of edits.\n", "\n", "Build a model with a quality metric *F1* of at least 0.75.\n", "\n", "**Instructions for the implementation of the project**\n", "\n", "1. Download and prepare data.\n", "2. Train different models.\n", "3. Draw conclusions.\n", "\n", "It is not necessary to use *BERT* to run the project, but you can try.\n", "\n", "**Data Description**\n", "\n", "The data is in the `toxic_comments.csv` file. The *text* column contains the text of the comment, and *toxic* is the target attribute."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preparation"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import time\n", "import warnings\n", "\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import re\n", "\n", "import spacy\n", "from nltk.stem import WordNetLemmatizer \n", "from nltk.corpus import stopwords\n", "import nltk\n", "\n", "from sklearn.linear_model import LogisticRegression\n", "from catboost import CatBoostRegressor\n", "\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.metrics import f1_score\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import RandomizedSearchCV\n", "\n", "from sklearn.model_selection import cross_val_score\n", "\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.utils import shuffle"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('~/toxic_comments.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's get acquainted with the data"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>Unnamed: 0</th>\n", "      <th>text</th>\n", "      <th>toxic</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>0</td>\n", "      <td>Explanation\\nWhy the edits made under my usern...</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>1</td>\n", "      <td>D'aww! He matches this background colour I'm s...</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2</td>\n", "      <td>Hey man, I'm really not trying to edit war. It...</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>3</td>\n", "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>4</td>\n", "      <td>You, sir, are my hero. Any chance you remember...</td>\n", "      <td>0</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   Unnamed: 0                                               text  toxic\n", "0           0  Explanation\\nWhy the edits made under my usern...      0\n", "1           1  D'aww! He matches this background colour I'm s...      0\n", "2           2  Hey man, I'm really not trying to edit war. It...      0\n", "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n", "4           4  You, sir, are my hero. Any chance you remember...      0"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["df.head()"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class 'pandas.core.frame.DataFrame'>\n", "RangeIndex: 159292 entries, 0 to 159291\n", "Data columns (total 3 columns):\n", " #   Column      Non-Null Count   Dtype \n", "---  ------      --------------   ----- \n", " 0   Unnamed: 0  159292 non-null  int64 \n", " 1   text        159292 non-null  object\n", " 2   toxic       159292 non-null  int64 \n", "dtypes: int64(2), object(1)\n", "memory usage: 3.6+ MB\n"]}], "source": ["df.info()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have English here, let's celebrate it!"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": ["0"]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["df.duplicated().sum()"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"data": {"text/plain": ["0    143106\n", "1     16186\n", "Name: toxic, dtype: int64"]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["df['toxic'].value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["class imbalance is visible"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's prepare the signs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Spacy lemmatization"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n", "\n", "def spacy_lemm(row):\n", "    doc = nlp(row)  \n", "    lemma = ' '.join([token.lemma_ for token in doc])\n", "    lemma = ''.join(re.sub(r'[^A-Za-z]',' ',lemma))\n", "    lemma = \" \".join(lemma.split())\n", "    return lemma"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["CPU times: user 16min 23s, sys: 7.25 s, total: 16min 30s\n", "Wall time: 16min 33s\n"]}], "source": ["%%time\n", "df['lemm']=df['text'].apply(spacy_lemm)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Very long lemmatization..."]}, {"cell_type": "code", "execution_count": 21, "metadata": {"scrolled": true}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>Unnamed: 0</th>\n", "      <th>text</th>\n", "      <th>toxic</th>\n", "      <th>lemm</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>0</td>\n", "      <td>Explanation\\nWhy the edits made under my usern...</td>\n", "      <td>0</td>\n", "      <td>explanation why the edit make under my usernam...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>1</td>\n", "      <td>D'aww! He matches this background colour I'm s...</td>\n", "      <td>0</td>\n", "      <td>d aww he match this background colour I be see...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>2</td>\n", "      <td>Hey man, I'm really not trying to edit war. It...</td>\n", "      <td>0</td>\n", "      <td>hey man I be really not try to edit war it be ...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>3</td>\n", "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n", "      <td>0</td>\n", "      <td>More I can not make any real suggestion on imp...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>4</td>\n", "      <td>You, sir, are my hero. Any chance you remember...</td>\n", "      <td>0</td>\n", "      <td>you sir be my hero any chance you remember wha...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["   Unnamed: 0                                               text  toxic  \\\n", "0           0  Explanation\\nWhy the edits made under my usern...      0   \n", "1           1  D'aww! He matches this background colour I'm s...      0   \n", "2           2  Hey man, I'm really not trying to edit war. It...      0   \n", "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n", "4           4  You, sir, are my hero. Any chance you remember...      0   \n", "\n", "                                                lemm  \n", "0  explanation why the edit make under my usernam...  \n", "1  d aww he match this background colour I be see...  \n", "2  hey man I be really not try to edit war it be ...  \n", "3  More I can not make any real suggestion on imp...  \n", "4  you sir be my hero any chance you remember wha...  "]}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}], "source": ["df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Preparing samples\n", "Let's break it down into training and test sets before tokinesis."]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["target = df['toxic']\n", "features = df['lemm']\n", "\n", "features_train, features_valid, target_train, target_valid = train_test_split(features, \n", "                                                                              target, \n", "                                                                              test_size=0.2, \n", "                                                                              random_state=12345)"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["train: 127433\n", "valid: 31859\n"]}], "source": ["print('train:',features_train.shape[0])\n", "print('valid:',features_valid.shape[0])"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"data": {"text/plain": ["97400     bushranger you be a GRASS with no sense of hum...\n", "4383      need administrative help I have be block iniqu...\n", "103680    I would also like to point out that he have us...\n", "38573        you can not block I you fuck retard BRB nigger\n", "128311    I believe that the frequency of the wave need ...\n", "Name: lemm, dtype: object"]}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": ["features_train.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Tokenization"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["[nltk_data] Downloading package stopwords to /Users/konn4/nltk_data...\n", "[nltk_data]   Package stopwords is already up-to-date!\n"]}], "source": ["nltk.download('stopwords')\n", "stop_words_my = set(stopwords.words('english'))"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["corpus_train = features_train\n", "corpus_valid = features_valid"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u0420\u0430\u0437\u043c\u0435\u0440 \u043c\u0430\u0442\u0440\u0438\u0446\u044b train: (127433, 137453)\n", "\u0420\u0430\u0437\u043c\u0435\u0440 \u043c\u0430\u0442\u0440\u0438\u0446\u044b train: (31859, 137453)\n", "CPU times: user 3.6 s, sys: 49.6 ms, total: 3.65 s\n", "Wall time: 3.65 s\n"]}], "source": ["%%time\n", "count_tf_idf = TfidfVectorizer(stop_words=stop_words_my) \n", "\n", "tf_idf_train = count_tf_idf.fit_transform(corpus_train) \n", "tf_idf_valid = count_tf_idf.transform(corpus_valid)\n", "\n", "print(\"\u0420\u0430\u0437\u043c\u0435\u0440 \u043c\u0430\u0442\u0440\u0438\u0446\u044b train:\", tf_idf_train.shape)\n", "print(\"\u0420\u0430\u0437\u043c\u0435\u0440 \u043c\u0430\u0442\u0440\u0438\u0446\u044b train:\", tf_idf_valid.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training\n", "### LogisticRegression on unbalanced classes"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["cv = KFold(n_splits=3, shuffle=True, random_state=12345)"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["F1 \u043d\u0430 CV 0.7089543561992754\n", "CPU times: user 6.64 s, sys: 1.83 s, total: 8.47 s\n", "Wall time: 4.76 s\n"]}], "source": ["%%time\n", "\n", "model_lr = LogisticRegression()\n", "\n", "train_f1 = cross_val_score(model_lr, \n", "                      tf_idf_train, \n", "                      target_train, \n", "                      cv=cv, \n", "                      scoring='f1')\n", "\n", "print('F1 \u043d\u0430 CV', train_f1.mean())"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["F1 valid 0.7497681320719717\n"]}], "source": ["model_lr.fit(tf_idf_train, target_train)\n", "\n", "prediction_lr_valid = model_lr.predict(tf_idf_valid)\n", "\n", "print('F1 valid', f1_score(target_valid, prediction_lr_valid))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### LogisticRegression with class_weight='balanced'"]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["F1 \u043d\u0430 CV 0.7472192456303787\n", "CPU times: user 6.85 s, sys: 1.88 s, total: 8.73 s\n", "Wall time: 4.91 s\n"]}], "source": ["%%time\n", "\n", "model_lr_bal = LogisticRegression(class_weight='balanced')\n", "\n", "train_f1 = cross_val_score(model_lr_bal, \n", "                      tf_idf_train, \n", "                      target_train, \n", "                      cv=cv, \n", "                      scoring='f1')\n", "\n", "print('F1 \u043d\u0430 CV', train_f1.mean())"]}, {"cell_type": "code", "execution_count": 36, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["F1 valid 0.750943396226415\n"]}], "source": ["model_lr_bal.fit(tf_idf_train, target_train)\n", "\n", "prediction_lr_valid = model_lr_bal.predict(tf_idf_valid)\n", "\n", "print('F1 valid', f1_score(target_valid, prediction_lr_valid))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### DecisionTree with class_weight='balanced'"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'class_weight': 'balanced', 'max_depth': 100, 'random_state': 12345}\n", "CPU times: user 4min 18s, sys: 784 ms, total: 4min 19s\n", "Wall time: 4min 19s\n"]}], "source": ["%%time\n", "\n", "model_dt = DecisionTreeClassifier()\n", "params = [{'max_depth':[10,30,50,100],\n", "                'random_state':[12345],\n", "                'class_weight':['balanced']}]\n", "\n", "gridsearch = GridSearchCV(model_dt, params, scoring='f1',cv=cv)\n", "gridsearch.fit(tf_idf_train, target_train)\n", "print(gridsearch.best_params_)"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"data": {"text/plain": ["0.6423755884378052"]}, "execution_count": 38, "metadata": {}, "output_type": "execute_result"}], "source": ["gridsearch.best_score_"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["DecisionTree F1 valid 0.6489563567362429\n"]}], "source": ["prediction_dt_valid = gridsearch.best_estimator_.predict(tf_idf_valid)\n", "\n", "print('DecisionTree F1 valid', f1_score(target_valid, prediction_dt_valid))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Let's try to balance classes through sample reduction"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["toxic_comments_train = df.iloc[target_train.index]\n", "\n", "target_train_0 = toxic_comments_train[toxic_comments_train['toxic'] == 0]['toxic']\n", "target_train_1 = toxic_comments_train[toxic_comments_train['toxic'] == 1]['toxic']"]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u0414\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 1 \u043a 8.835828959555418\n"]}], "source": ["print('\u0414\u0438\u0441\u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 1 \u043a', target_train_0.shape[0]/target_train_1.shape[0])"]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": ["target_train_0_downsample = target_train_0.sample(target_train_1.shape[0],random_state=12345)\n", "target_train_downsample = pd.concat([target_train_0_downsample, target_train_1])\n", "\n", "features_train_downsample = df.iloc[target_train_downsample.index]\n", "\n", "features_train_downsample, target_train_downsample = shuffle(features_train_downsample,\n", "                                                             target_train_downsample,\n", "                                                             random_state=12345)\n", "\n", "features_train_downsample = count_tf_idf.transform(features_train_downsample['lemm'].values.astype('U'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Logistic regression on a reduced sample"]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["F1 \u043d\u0430 CV 0.888140831709479\n", "CPU times: user 3.65 s, sys: 1.75 s, total: 5.4 s\n", "Wall time: 1.66 s\n"]}], "source": ["%%time\n", "\n", "model_lr_ds = LogisticRegression()\n", "\n", "train_f1 = cross_val_score(model_lr_ds, \n", "                      features_train_downsample, \n", "                      target_train_downsample, \n", "                      cv=cv, \n", "                      scoring='f1')\n", "\n", "print('F1 \u043d\u0430 CV', train_f1.mean())"]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["F1 valid downsampled 0.7006211180124224\n"]}], "source": ["model_lr_ds.fit(features_train_downsample, target_train_downsample)\n", "\n", "prediction_lr_valid_ds = model_lr_ds.predict(tf_idf_valid)\n", "\n", "print('F1 valid downsampled', f1_score(target_valid, prediction_lr_valid_ds))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## conclusions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["| Model | F1 train | F1 valid | pass |\n", "| ----------- | ----------- | -------- | --- |\n", "| LogisticRegression | 0.7089543561992754 | 0.7497681320719717 | no |\n", "| LogisticRegression balanced | 0.7472192456303787 | 0.750943396226415 | Yes |\n", "| LogisticRegression downsamled | 0.888140831709479 | 0.7006211180124224 | no |\n", "| DecisionTree | 0.6423755884378052 | 0.6489563567362429 | no |"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Conclusion\n", "It was possible to achieve the F1 metric of 0.750943396226415 on the validation dataset by increasing the training set and leaving only the validation one, as well as by making lemmatization."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Checklist"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- [x] Jupyter Notebook open\n", "- [x] All code runs without errors\n", "- [x] Cells with code are arranged in execution order\n", "- [x] Data loaded and prepared\n", "- [x] Models trained\n", "- [x] Metric value *F1* not less than 0.75\n", "- [x] Conclusions written"]}], "metadata": {"ExecuteTimeLog": [{"duration": 1702, "start_time": "2022-10-10T10:38:45.793Z"}, {"duration": 3, "start_time": "2022-10-10T10:38:47.497Z"}, {"duration": 3292, "start_time": "2022-10-10T10:38:47.501Z"}, {"duration": 13, "start_time": "2022-10-10T10:38:50.796Z"}, {"duration": 11, "start_time": "2022-10-10T10:39:08.335Z"}, {"duration": 10, "start_time": "2022-10-10T10:44:21.384Z"}, {"duration": 123, "start_time": "2022-10-10T10:44:38.821Z"}, {"duration": 247, "start_time": "2022-10-10T10:44:54.466Z"}, {"duration": 229, "start_time": "2022-10-10T10:44:58.457Z"}, {"duration": 6, "start_time": "2022-10-10T10:45:06.845Z"}, {"duration": 12, "start_time": "2022-10-10T12:38:12.840Z"}, {"duration": 2203, "start_time": "2022-10-11T12:24:50.602Z"}, {"duration": 3, "start_time": "2022-10-11T12:34:47.476Z"}, {"duration": 4, "start_time": "2022-10-11T12:34:53.627Z"}, {"duration": 2, "start_time": "2022-10-11T12:34:53.991Z"}, {"duration": 3341, "start_time": "2022-10-11T12:34:54.283Z"}, {"duration": 10, "start_time": "2022-10-11T12:34:57.626Z"}, {"duration": 212, "start_time": "2022-10-11T12:34:57.637Z"}, {"duration": 5, "start_time": "2022-10-11T12:34:57.851Z"}, {"duration": 2, "start_time": "2022-10-11T12:35:00.324Z"}, {"duration": 102, "start_time": "2022-10-11T12:35:01.023Z"}, {"duration": 1412, "start_time": "2022-10-11T13:29:33.576Z"}, {"duration": 3, "start_time": "2022-10-11T13:29:35.000Z"}, {"duration": 1111, "start_time": "2022-10-11T13:29:35.004Z"}, {"duration": 20, "start_time": "2022-10-11T13:29:36.775Z"}, {"duration": 30, "start_time": "2022-10-11T13:29:53.407Z"}, {"duration": 214, "start_time": "2022-10-11T13:29:59.460Z"}, {"duration": 6, "start_time": "2022-10-11T13:29:59.944Z"}, {"duration": 3, "start_time": "2022-10-11T13:30:11.015Z"}, {"duration": 1614, "start_time": "2022-10-11T13:36:31.657Z"}, {"duration": 3, "start_time": "2022-10-11T13:36:33.273Z"}, {"duration": 789, "start_time": "2022-10-11T13:36:33.277Z"}, {"duration": 30, "start_time": "2022-10-11T13:36:34.069Z"}, {"duration": 31, "start_time": "2022-10-11T13:36:34.101Z"}, {"duration": 230, "start_time": "2022-10-11T13:36:34.134Z"}, {"duration": 5, "start_time": "2022-10-11T13:36:34.366Z"}, {"duration": 5, "start_time": "2022-10-11T13:36:34.385Z"}, {"duration": 3, "start_time": "2022-10-11T13:36:47.615Z"}, {"duration": 639, "start_time": "2022-10-11T13:37:06.475Z"}, {"duration": 3, "start_time": "2022-10-11T13:37:20.834Z"}, {"duration": 1070, "start_time": "2022-10-11T13:37:21.744Z"}, {"duration": 4, "start_time": "2022-10-11T13:37:57.766Z"}, {"duration": 4, "start_time": "2022-10-11T13:37:59.691Z"}, {"duration": 3, "start_time": "2022-10-11T13:38:03.347Z"}, {"duration": 1491, "start_time": "2022-10-11T14:00:56.167Z"}, {"duration": 3, "start_time": "2022-10-11T14:00:57.659Z"}, {"duration": 835, "start_time": "2022-10-11T14:00:57.663Z"}, {"duration": 11, "start_time": "2022-10-11T14:00:58.500Z"}, {"duration": 31, "start_time": "2022-10-11T14:00:58.512Z"}, {"duration": 238, "start_time": "2022-10-11T14:00:58.544Z"}, {"duration": 13, "start_time": "2022-10-11T14:00:58.783Z"}, {"duration": 9, "start_time": "2022-10-11T14:00:58.798Z"}, {"duration": 10, "start_time": "2022-10-11T14:00:58.809Z"}, {"duration": 4046, "start_time": "2022-10-11T14:35:35.603Z"}, {"duration": 4867, "start_time": "2022-10-11T14:35:47.433Z"}, {"duration": 6, "start_time": "2022-10-11T14:35:59.501Z"}, {"duration": 5, "start_time": "2022-10-11T14:36:10.253Z"}, {"duration": 11, "start_time": "2022-10-11T14:36:15.710Z"}, {"duration": 706, "start_time": "2022-10-11T14:36:34.897Z"}, {"duration": 638, "start_time": "2022-10-11T14:36:43.654Z"}, {"duration": 634, "start_time": "2022-10-11T14:37:01.997Z"}, {"duration": 6, "start_time": "2022-10-11T14:40:07.459Z"}, {"duration": 3119, "start_time": "2022-10-11T14:40:17.354Z"}, {"duration": 3124, "start_time": "2022-10-11T14:40:26.872Z"}, {"duration": 3041, "start_time": "2022-10-11T14:41:37.568Z"}, {"duration": 44, "start_time": "2022-10-11T17:00:22.485Z"}, {"duration": 52, "start_time": "2022-10-12T17:44:48.776Z"}, {"duration": 6, "start_time": "2022-10-12T17:45:01.362Z"}, {"duration": 1923, "start_time": "2022-10-12T17:45:06.706Z"}, {"duration": 4, "start_time": "2022-10-12T17:45:10.416Z"}, {"duration": 3, "start_time": "2022-10-12T17:45:25.037Z"}, {"duration": 9, "start_time": "2022-10-12T17:45:25.042Z"}, {"duration": 5, "start_time": "2022-10-12T17:45:25.053Z"}, {"duration": 2253, "start_time": "2022-10-12T17:45:25.060Z"}, {"duration": 91, "start_time": "2022-10-12T17:45:27.315Z"}, {"duration": 26, "start_time": "2022-10-12T17:45:27.409Z"}, {"duration": 265, "start_time": "2022-10-12T17:45:27.437Z"}, {"duration": 6, "start_time": "2022-10-12T17:45:27.704Z"}, {"duration": 4, "start_time": "2022-10-12T17:45:27.711Z"}, {"duration": 5, "start_time": "2022-10-12T17:45:27.717Z"}, {"duration": 103060, "start_time": "2022-10-12T17:45:29.055Z"}, {"duration": 141, "start_time": "2022-10-12T17:50:03.571Z"}, {"duration": 4, "start_time": "2022-10-12T17:50:04.390Z"}, {"duration": 19846, "start_time": "2022-10-12T17:50:07.154Z"}, {"duration": 9, "start_time": "2022-10-12T17:50:27.002Z"}, {"duration": 54, "start_time": "2022-10-12T17:51:52.931Z"}, {"duration": 5, "start_time": "2022-10-12T17:51:55.536Z"}, {"duration": 6, "start_time": "2022-10-12T17:51:55.736Z"}, {"duration": 168, "start_time": "2022-10-12T17:51:56.101Z"}, {"duration": 2330, "start_time": "2022-10-12T17:51:56.479Z"}, {"duration": 9861, "start_time": "2022-10-12T17:51:58.812Z"}, {"duration": 13, "start_time": "2022-10-12T17:52:08.675Z"}, {"duration": 30, "start_time": "2022-10-12T17:52:54.608Z"}, {"duration": 3063, "start_time": "2022-10-12T17:52:54.641Z"}, {"duration": 3, "start_time": "2022-10-12T17:52:57.706Z"}, {"duration": 2554, "start_time": "2022-10-12T17:52:57.711Z"}, {"duration": 12, "start_time": "2022-10-12T17:53:00.268Z"}, {"duration": 55, "start_time": "2022-10-12T17:53:00.283Z"}, {"duration": 329, "start_time": "2022-10-12T17:53:00.340Z"}, {"duration": 6, "start_time": "2022-10-12T17:53:00.671Z"}, {"duration": 7, "start_time": "2022-10-12T17:53:00.679Z"}, {"duration": 23, "start_time": "2022-10-12T17:53:00.687Z"}, {"duration": 126402, "start_time": "2022-10-12T17:53:00.712Z"}, {"duration": 154, "start_time": "2022-10-12T17:55:07.116Z"}, {"duration": 3, "start_time": "2022-10-12T17:55:07.272Z"}, {"duration": 19244, "start_time": "2022-10-12T17:55:07.277Z"}, {"duration": 9, "start_time": "2022-10-12T17:55:26.522Z"}, {"duration": 40, "start_time": "2022-10-12T17:55:26.532Z"}, {"duration": 5, "start_time": "2022-10-12T17:55:26.574Z"}, {"duration": 30, "start_time": "2022-10-12T17:55:26.580Z"}, {"duration": 131, "start_time": "2022-10-12T17:55:26.612Z"}, {"duration": 1945, "start_time": "2022-10-12T17:55:26.745Z"}, {"duration": 9237, "start_time": "2022-10-12T17:55:28.691Z"}, {"duration": 21, "start_time": "2022-10-12T17:55:37.931Z"}, {"duration": 4, "start_time": "2022-10-12T17:55:42.489Z"}, {"duration": 100689, "start_time": "2022-10-12T17:55:43.341Z"}, {"duration": 45887, "start_time": "2022-10-12T17:57:24.034Z"}, {"duration": 72701, "start_time": "2022-10-12T17:58:09.923Z"}, {"duration": 41487, "start_time": "2022-10-12T17:59:22.626Z"}, {"duration": 287792, "start_time": "2022-10-12T18:00:04.115Z"}, {"duration": 4, "start_time": "2022-10-12T18:04:51.909Z"}, {"duration": 38, "start_time": "2022-10-12T18:04:51.914Z"}, {"duration": 38, "start_time": "2022-10-12T18:04:51.953Z"}, {"duration": 3, "start_time": "2022-10-12T18:04:52.009Z"}, {"duration": 17, "start_time": "2022-10-12T18:13:47.465Z"}, {"duration": 1768, "start_time": "2022-10-12T18:13:53.547Z"}, {"duration": 3, "start_time": "2022-10-12T18:13:57.673Z"}, {"duration": 2260, "start_time": "2022-10-12T18:13:58.221Z"}, {"duration": 14, "start_time": "2022-10-12T18:14:00.483Z"}, {"duration": 29, "start_time": "2022-10-12T18:14:00.663Z"}, {"duration": 259, "start_time": "2022-10-12T18:14:01.008Z"}, {"duration": 5, "start_time": "2022-10-12T18:14:01.269Z"}, {"duration": 3, "start_time": "2022-10-12T18:14:04.904Z"}, {"duration": 3, "start_time": "2022-10-12T18:14:05.265Z"}, {"duration": 166, "start_time": "2022-10-12T18:14:28.513Z"}, {"duration": 5, "start_time": "2022-10-12T18:14:28.693Z"}, {"duration": 18201, "start_time": "2022-10-12T18:14:28.863Z"}, {"duration": 8, "start_time": "2022-10-12T18:14:47.065Z"}, {"duration": 50, "start_time": "2022-10-12T18:14:47.074Z"}, {"duration": 3, "start_time": "2022-10-12T18:14:47.126Z"}, {"duration": 7, "start_time": "2022-10-12T18:14:47.131Z"}, {"duration": 197, "start_time": "2022-10-12T18:14:47.139Z"}, {"duration": 1891, "start_time": "2022-10-12T18:14:47.338Z"}, {"duration": 8681, "start_time": "2022-10-12T18:14:49.230Z"}, {"duration": 53, "start_time": "2022-10-12T18:15:06.469Z"}, {"duration": 4, "start_time": "2022-10-12T18:15:07.033Z"}, {"duration": 1161, "start_time": "2022-10-12T18:15:07.625Z"}, {"duration": 192, "start_time": "2022-10-12T18:15:13.662Z"}, {"duration": 9406, "start_time": "2022-10-12T18:15:14.018Z"}, {"duration": 3, "start_time": "2022-10-12T18:19:28.982Z"}, {"duration": 14, "start_time": "2022-10-12T18:19:44.740Z"}, {"duration": 86330, "start_time": "2022-10-12T18:20:03.283Z"}, {"duration": 41404, "start_time": "2022-10-12T18:21:29.615Z"}, {"duration": 72000, "start_time": "2022-10-12T18:22:11.022Z"}, {"duration": 41110, "start_time": "2022-10-12T18:24:05.912Z"}, {"duration": 289414, "start_time": "2022-10-12T18:25:36.959Z"}, {"duration": 4, "start_time": "2022-10-12T18:30:26.375Z"}, {"duration": 57, "start_time": "2022-10-12T18:30:26.381Z"}, {"duration": 52370, "start_time": "2022-10-12T18:30:26.440Z"}, {"duration": 10221, "start_time": "2022-10-12T18:31:18.812Z"}, {"duration": 15, "start_time": "2022-10-12T18:31:29.035Z"}, {"duration": 75, "start_time": "2022-10-12T18:31:29.052Z"}, {"duration": 23, "start_time": "2022-10-12T18:31:29.130Z"}, {"duration": 96, "start_time": "2022-10-12T18:31:29.155Z"}, {"duration": 4172, "start_time": "2022-10-13T11:04:08.256Z"}, {"duration": 359, "start_time": "2022-10-13T11:07:12.888Z"}, {"duration": 477, "start_time": "2022-10-13T11:07:38.908Z"}, {"duration": 25, "start_time": "2022-10-13T11:07:55.927Z"}, {"duration": 425, "start_time": "2022-10-13T11:08:06.070Z"}, {"duration": 11, "start_time": "2022-10-13T11:08:06.877Z"}, {"duration": 4, "start_time": "2022-10-13T11:08:23.986Z"}, {"duration": 3, "start_time": "2022-10-13T11:08:24.555Z"}, {"duration": 3013, "start_time": "2022-10-13T11:08:24.894Z"}, {"duration": 10, "start_time": "2022-10-13T11:08:27.909Z"}, {"duration": 28, "start_time": "2022-10-13T11:08:27.920Z"}, {"duration": 206, "start_time": "2022-10-13T11:08:27.950Z"}, {"duration": 7, "start_time": "2022-10-13T11:08:28.158Z"}, {"duration": 429, "start_time": "2022-10-13T11:08:38.146Z"}, {"duration": 999, "start_time": "2022-10-13T11:08:55.467Z"}, {"duration": 1038002, "start_time": "2022-10-13T11:09:21.603Z"}, {"duration": 6, "start_time": "2022-10-13T11:26:39.607Z"}, {"duration": 470, "start_time": "2022-10-13T11:39:52.810Z"}, {"duration": 65, "start_time": "2022-10-13T12:49:54.503Z"}], "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.13"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": true, "title_cell": "\u0421\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0435", "title_sidebar": "Contents", "toc_cell": true, "toc_position": {"height": "calc(100% - 180px)", "left": "10px", "top": "150px", "width": "165px"}, "toc_section_display": true, "toc_window_display": true}}, "nbformat": 4, "nbformat_minor": 2}